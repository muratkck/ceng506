{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "iGJKKTTli4V-",
   "metadata": {
    "id": "iGJKKTTli4V-"
   },
   "source": [
    "# Transfer Learning for Animal Image Classification\n",
    "\n",
    "In this homework assignment, you will develop and refine a neural network model for classifying animal images from a provided dataset containing five classes: \"alpaca\", \"bear\", \"elephant\", \"leopard\", and \"zebra\". The dataset (alpaca_and_others_dataset.zip) is available on MS Teams under the path General/Class Materials/datasets. The primary goal is to improve upon a baseline model through experimentation with different architectures, hyperparameters, and techniques like transfer learning and data augmentation.\n",
    "\n",
    "## Baseline Model\n",
    "\n",
    "Select a neural network architecture and a set of hyperparameters to train the network. You can split the dataset as train and validation, no need for a separate test set. Refer to the resulting model as \"baseline model.\" Note that this baseline model's performance may be low but we should be able to observe improvement (decrease in loss, increase in validation accuracy) after the training.\n",
    "\n",
    "## Controlled Experiments\n",
    "\n",
    "Apply changes that can potentially improve the baseline model to obtain several candidate models each of which differs from the baseline model by a single modification, for example:\n",
    "\n",
    "- Transfer learning rather than training from scratch (mandatory)\n",
    "- Using a different architecture\n",
    "- Data augmentation\n",
    "\n",
    "Make sure that you perform \"controlled experiments.\" Clearly specify what is changed in each candidate model. Train them for the same number of epochs, and compare all the results by plotting their training and validation losses with respect to epochs. You can smooth the curves if needed. Share your findings and comments.\n",
    "\n",
    "## Proposed Model\n",
    "\n",
    "Propose one new model by observing the previous results. You can combine multiple changes together (e.g. a combination of transfer learning and data augmentation if you observe that both of these improve the performance). Briefly share your conclusions as a list of facts and comments.\n",
    "\n",
    "## Repeatability\n",
    "\n",
    "In your homework ensure repeatability of your results by implementing the following practices:\n",
    "\n",
    "- **Setting Random Seeds:** Initialize random number generator seeds for ALL randomness modules directly or indirectly used (e.g. `random`, `tensorflow.random` etc.) to ensure reproducibility of random processes.\n",
    "\n",
    "- **Environment Reproducibility:** At the beginning of your code specify the versions of dependencies (e.g., TensorFlow) used in your environment. This ensures that the same environment can be recreated to reproduce the results.\n",
    "\n",
    "- **Deterministic Processes:** Avoid using processes whose results rely on external factors such as operating system or computation power. For example, ensure deterministic file listing (e.g., use `sorted(os.listdir())` instead of `os.listdir())` and fixed number of epochs rather than using a predetermined time budget such as 1 hour for training.\n",
    "\n",
    "## Rules\n",
    "\n",
    "You are expected to use TensorFlow-Keras library and submit a single Jupyter/Colab Notebook. Please make sure to show each step clearly in your notebook without leaving any room for doubt and without any exception. Please add visualizations, analysis, or comments if necessary. Please note that performing transfer learning in at least one of the candidate models is mandatory. There are many TF-Keras transfer learning examples online. You can examine them. There must be sufficient amount of individual work in the solution: Selection of models, selection of layers for transfer learning, data augmentation options, selection of hyperparameters, regularization options etc. must differ from the available solutions. Groupwork is NOT allowed. You will prepare and submit homeworks individually.\n",
    "\n",
    "For making the homework evaluation easier, please make sure the submitted notebook includes separate sections for different tasks and displays all the outputs without the need for running the code!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e46e73f",
   "metadata": {},
   "source": [
    "# Animal Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba867e",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c431b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.10' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/murat/AppData/Local/Microsoft/WindowsApps/python3.12.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76560f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print versions for reproducibility\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d66f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9cbda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "dataset_dir = \"/c:/Users/murat/Desktop/term_8/CENG506/HWs/hw2/dataset\"\n",
    "output_dir = \"/c:/Users/murat/Desktop/term_8/CENG506/HWs/hw2/splitted_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03963cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and validation directories\n",
    "train_dir = os.path.join(output_dir, \"train\")\n",
    "val_dir = os.path.join(output_dir, \"val\")\n",
    "\n",
    "# Make output directories\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ae8469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split ratio\n",
    "val_split = 0.2\n",
    "\n",
    "# Process each class\n",
    "classes = [\"alpaca\", \"bear\", \"elephant\", \"leopard\", \"zebra\"]\n",
    "\n",
    "for cls in classes:\n",
    "    # Create class subdirectories\n",
    "    os.makedirs(os.path.join(train_dir, cls), exist_ok=True)\n",
    "    os.makedirs(os.path.join(val_dir, cls), exist_ok=True)\n",
    "    \n",
    "    # Get all images for this class\n",
    "    class_path = os.path.join(dataset_dir, cls)\n",
    "    images = sorted([img for img in os.listdir(class_path) if img.endswith(('.jpg', '.JPEG', '.png'))])\n",
    "    \n",
    "    # Split into train and validation\n",
    "    train_images, val_images = train_test_split(images, test_size=val_split, random_state=SEED)\n",
    "    \n",
    "    # Copy images to respective directories\n",
    "    for img in train_images:\n",
    "        src = os.path.join(class_path, img)\n",
    "        dst = os.path.join(train_dir, cls, img)\n",
    "        shutil.copy(src, dst)\n",
    "    \n",
    "    for img in val_images:\n",
    "        src = os.path.join(class_path, img)\n",
    "        dst = os.path.join(val_dir, cls, img)\n",
    "        shutil.copy(src, dst)\n",
    "    \n",
    "    print(f\"Class {cls}: {len(train_images)} training images, {len(val_images)} validation images\")\n",
    "\n",
    "print(\"\\nDataset split complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67928510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "for split in [\"train\", \"val\"]:\n",
    "    print(f\"\\n{split.capitalize()} set:\")\n",
    "    split_path = os.path.join(output_dir, split)\n",
    "    for cls in classes:\n",
    "        cls_path = os.path.join(split_path, cls)\n",
    "        num_images = len(os.listdir(cls_path))\n",
    "        print(f\"  {cls}: {num_images} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6e52e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image parameters\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create data generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create data generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70008072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_model():\n",
    "    model = models.Sequential([\n",
    "        # First Convolutional Block\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Third Convolutional Block\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Flatten and Dense Layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(5, activation='softmax')  # 5 classes\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and compile the model\n",
    "baseline_model = create_baseline_model()\n",
    "baseline_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d73af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "EPOCHS = 20\n",
    "\n",
    "# Train the model\n",
    "history = baseline_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_generator.samples // BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1090bc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    # Plot training & validation accuracy\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    # Plot training & validation loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training history\n",
    "plot_training_history(history)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1q9LGI44czUwvv_oxzz6ucx_dhUVBcblj",
     "timestamp": 1713448460014
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
